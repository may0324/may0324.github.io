---
layout:     post
title:      基于caffe的量化模型训练与部署——训练篇
subtitle:    "\"模型量化\""
date:       2019-05-15
author:     MOOMO
header-img: img/IMG_0238.jpg
catalog: true
tags:
    - CNN 模型量化 Caffe
---

### 为什么需要量化？
我们知道，cnn网络的前向计算瓶颈主要集中在卷积层，而卷积层的实质是大量的浮点数相乘、相加等运算操作，大量的浮点数计算限制了模型在低处理器或移动端等设备中的部署。如果能将浮点运算转换为整形运算，则cnn模型的前向处理速度将达到质的提升。
### 为什么量化有用？
关于深度神经网络的一个非常有意思的发现是：即使输入中包含大量的噪声，神经网络依然能处理的很好。深度网络的一个神奇特质是它们倾向于很好地应对输入中的高噪声。如果您考虑识别您刚拍摄的照片中的物体，网络必须忽略所有CCD噪音，光照变化以及它与之前看到的训练样例之间的其他非本质差异，并关注重要事项相似之处。这种能力意味着它们似乎将低精度计算视为另一种噪声源，并且即使使用包含较少信息的数字格式仍能产生准确的结果。
### 如何量化？
由于网络中主要的计算集中在卷积层，这里主要以卷积层的量化为例讲解。
量化需要完成的是将浮点型的输入值和网络参数值分别映射到一定范围内的整型数。假设我们要将网络参数和输入值量化为kbit，即对应的整型数值表述范围为$2^k$个。一种最常用的量化方法就是线性量化，即将一定范围内的浮点型数均匀量化成$2^k$个区间，每个区间对应一个整形数值，如下表为将[-1, 1]范围内的浮点数线性量化为2比特4个区间：

```
------------------------------
input         |        output
------------------------------
[-1.0,-0.5)   |          -2
[-0.5,0)      |          -1
[0,0.5)       |          0
[0.5,1.0]      |          1
------------------------------	
```
均匀量化需要明确被量化的浮点型数值的范围，通过对卷积层的输入和参数特性分析发现，这一数值范围其实是可以大致确定的：
- 卷积层输入范围
对于卷积层，其输入为上一层的输出，通常包含两种情况，一种是输入图像，另一种是经过激活函数的前一层卷积结果，所以卷积层的输入值的取值范围通常为非负数，故对应的量化到[0,$2^k$-1]中。
$I\in[0,+∞]$     ——> $Iq\in\{0,1,2,...2^k-1\}$
- 卷积层参数范围
而通过对常见网络的卷积层参数的可视化发现，卷积层参数基本呈现以0为中心轴，对称分布的状态，且最大值和最小值的绝对值基本接近，如下图所示，因此可以将参数量化到[-($2^k/2$), $2^k/2-1$]中。
$W\in[-min,+max]$     ——> $Wq\in\{-(2^k/2),-(2^k/2)+1,...2^k/2-1\}$
![在这里插入图片描述](https://img-blog.csdnimg.cn/20190515113017239.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L21heTAzMjQ=,size_16,color_FFFFFF,t_70)

通过统计大量的网络参数和输入的数值分布，在量化时我们可以确定输入和参数的大致正负边界（如网络参数基本落在[-1,1]区间，输入值基本落在[0,5]之间，对于大多数网络基本满足这一情况），对于落在边界外的少数数值，直接进行截断即可，这带来的噪声值是相对微弱的。

### 量化公式
我们知道，卷积层的前向计算公式可以表示为输入和卷积层参数的卷积运算，简化为：
$$O=I*W$$对于量化计算，我们希望找到一组对应的量化值$I^q$和$W^q$，使其满足：
$$O=I*W≈\alpha I^q*\beta W^q=\alpha \beta (I^q*W^q)$$上式中，$\alpha$和$\beta$是浮点型标量系数。
因此浮点型的卷积操作可以退化成对应量化值$I^q$和$W^q$的整型卷积操作，大大提升计算的效率。
- 卷积层输入量化
前面已经说过，我们可以采用最简单的线性均匀量化来确定上面所说的量化值，输入$I$被量化到[0,$2^k$-1]区间，由均匀量化公式可得重建公式为：
$$ I≈ \widetilde{I}=[\cfrac{\overline{I}-I_{min}}{Qs_I}]*Qs_I+I_{min} \tag 1 $$ 式中，$Qs_I$为量化区间步长,[.]为取整公式 $$Qs_I = \cfrac{I_{max}-I_{min}}{2^k-1} $$  $\overline{I}$为截断的输入值
$$
\qquad  \qquad  \qquad I_{min}   \qquad if \, I<I_{min} \\
\overline{I}=\{ I \qquad others  \\
\qquad \qquad \qquad I_{max} \qquad if \, I>I_{max}$$
特别的，上面提到，通常卷积层的输入为非负数，因此有$I_{min}=0$，上面的重建公式简化为
$$ I≈ \widetilde{I}=[\cfrac{\overline{I}}{Qs_I}]*Qs_I=I^q*Qs_I   $$ 
则有，输入$I$对应的量化值为
$$I^q=[\cfrac{\overline{I}}{Qs_I}]，\alpha=Qs_I \tag 2$$

- 卷积层参数量化
同样的，对于每个$C*K*K$维的卷积核参数，被均匀量化到[-($2^k/2$), $2^k/2-1$]区间中，其量化后到重建公式为：
$$ W_n≈\widetilde{W_n}=[\cfrac{\overline{W_n}}{Qs_Wn}]*Qs_Wn \tag 3 $$ 式中，$n \in\{1,2,...N \}$，N为输出通道数。$Qs_Wn$为量化区间步长,[.]为取整公式 $$Qs_Wn = \cfrac{2*Wn_{max}}{2^k-2} $$  $Wn_{max}=|W_n|_{max}$ , $\overline{W_n}$为截断的参数值
$$
\qquad  \qquad  \qquad \qquad-Wn_{max}   \qquad if \, W_n<-Wn_{max} \\
\overline{W_n}=\{ W_n \qquad others  \\
\qquad \qquad \qquad \qquad Wn_{max} \qquad if \, W_n>Wn_{max}$$
参数$Wn$对应的量化值为
$$W_n^q=[\cfrac{\overline{W_n}}{Qs_Wn}]，\beta=Qs_Wn \tag 4$$

### 训练算法
量化训练的卷积层前向和后向计算基本改动不大，唯一的差别是用量化重建后的输入值和参数值代替原始值做计算，从而在训练的过程中最小化量化引起的误差，
- 前向计算
-----
：1.根据量化重建公式(1)将输入$I$表示为量化后的重建值$\widetilde{I}$

：2. for each n in N:
		  根据量化重建公式(3)将卷积层参数$W_n$表示为量化后的重建值$\widetilde{W_n}$
	
：3.用重建后的输入值和参数进行值卷积计算 $O=\widetilde{I}*\widetilde{W_n}$

----
- 反向传播
- ---
： 1.计算$\cfrac{\partial L}{\partial bias}$
： 2.通过$\widetilde{I}$计算$\cfrac{\partial L}{\partial\widetilde{W}}$，同时将被截断的$\widetilde{W}$对应的梯度置零，避免其梯度传播
： 3.通过$\widetilde{W}$计算$\cfrac{\partial L}{\partial\widetilde{I}}$，同时将被截断的$\widetilde{I}$对应的梯度置零，避免其梯度传播

-----

### 在caffe中实现量化
新增加conv_quantized_layer.cu/cpp/hpp
主要函数：
量化输入值

```
template <typename Dtype>
__global__ void binarize_kernel(const Dtype *x, int n, float maxcutoff, float mincutoff, float quantStep, Dtype *binary)
{
    int i = (blockIdx.x + blockIdx.y*gridDim.x) * blockDim.x + threadIdx.x;
    if (i >= n) return;

      binary[i] = (x[i] >= maxcutoff) ? maxcutoff: ((x[i] < mincutoff) ? mincutoff : x[i]);
      Dtype temp = binary[i] - mincutoff;
      binary[i] = (round(temp / quantStep)) * quantStep + mincutoff;
}

template <typename Dtype>
void binarize_input_gpu(const Dtype *input, int n, int size, float maxcutoff, float mincutoff, int bit, Dtype *binary)
{
    //float maxcutoff = MAXCUTOFF_A;
    //float mincutoff = MINCUTOFF_A;
    float bitNum = pow(2,bit);
    float quantStep =  (maxcutoff - mincutoff) / (bitNum - 1);

      binarize_kernel<<<cuda_gridsize(n * size), BLOCK>>>(input, n * size, maxcutoff, mincutoff, quantStep, binary);
    
    check_error(cudaPeekAtLastError());
}
```

量化参数值

```
template <typename Dtype>
__global__ void binarize_weights_kernel(const Dtype *weights, int n, int size, float maxcutoff, float bitNum, Dtype *binary)
{
    int f = (blockIdx.x + blockIdx.y*gridDim.x) * blockDim.x + threadIdx.x;
    if (f >= n) return;
    int i = 0;

    Dtype weight_temp;
    Dtype weight_max = -1.0f;
    Dtype quantStep = 0.0f;
    //float cutoffNew = 0.0f;
    for(i = 0; i < size; ++i)
    {
        weight_temp = fabs(weights[f*size + i]);
        weight_max = (weight_temp > weight_max) ? weight_temp : weight_max;
    }
    weight_max = (weight_max > maxcutoff) ? maxcutoff : weight_max;
    quantStep = 2 * weight_max / (bitNum - 2);
    //cutoffNew = weight_max - quantStep;

    for(i = 0; i < size; ++i)
    {
       weight_temp = weights[f*size + i];
       weight_temp = (weight_temp >= weight_max) ? (weight_max): ((weight_temp < -weight_max) ? -weight_max : weight_temp);
       binary[f*size + i] = round(weight_temp / quantStep) * quantStep;
    }
}

template <typename Dtype>
void binarize_weights_gpu(const Dtype *weights, int n, int size, float maxcutoff, float mincutoff, int bit, Dtype *binary)//n:kernel number size:c*kh*kw
{
    //printf("BLOCK = %d\n",BLOCK);
    //float maxcutoff = MAXCUTOFF_W;
    //float mincutoff = MINCUTOFF_W;
    float bitNum = pow(2,bit);

     binarize_weights_kernel<<<cuda_gridsize(n), BLOCK>>>(weights, n, size, maxcutoff, bitNum, binary);
    
    check_error(cudaPeekAtLastError());
}
```

前向计算

```
template <typename Dtype>
void ConvQuantizedLayer<Dtype>::Forward_gpu(const vector<Blob<Dtype>*>& bottom,
      const vector<Blob<Dtype>*>& top)
{


    //quantized weight
    const Dtype* weight = this->blobs_[0]->gpu_data();
    Dtype* xnor_weight = this->xnor_blobs_[0]->mutable_gpu_data();
    binarize_weights_gpu(weight, conv_out_channels_, kernel_dim_, MAXCUTOFF_W, MINCUTOFF_W, WBIT, xnor_weight);

    for (int i = 0; i < bottom.size(); ++i) {
        const Dtype* bottom_data = bottom[i]->gpu_data();

        //quantizing input
        Dtype* xnor_bottom_data = this->xnor_bottom_[i]->mutable_gpu_data();
        binarize_input_gpu(bottom_data, num_, bottom_dim_, MAXCUTOFF_A, MINCUTOFF_A, ABIT, xnor_bottom_data);

        Dtype* top_data = top[i]->mutable_gpu_data();
        for (int n = 0; n < this->num_; ++n) {
            this->forward_gpu_gemm(xnor_bottom_data + n * this->bottom_dim_,
                    xnor_weight, top_data + n * this->top_dim_);
            if (this->bias_term_) {
                const Dtype* bias = this->blobs_[1]->gpu_data();
                this->forward_gpu_bias(top_data + n * this->top_dim_, bias);
            }
        }
    }
}
```

后向传播

```
template <typename Dtype>
void ConvQuantizedLayer<Dtype>::Backward_gpu(const vector<Blob<Dtype>*>& top,
      const vector<bool>& propagate_down, const vector<Blob<Dtype>*>& bottom)
{
  const Dtype* weight = this->blobs_[0]->gpu_data();
  Dtype* weight_diff = this->blobs_[0]->mutable_gpu_diff();
  const Dtype* xnor_weight = this->xnor_blobs_[0]->gpu_data();

  for (int i = 0; i < top.size(); ++i)
  {
    const Dtype* top_diff = top[i]->gpu_diff();
    // d(loss)/d(bias)
    if (this->bias_term_ && this->param_propagate_down_[1])
    {
      Dtype* bias_diff = this->blobs_[1]->mutable_gpu_diff();
      for (int n = 0; n < this->num_; ++n)
      {
        this->backward_gpu_bias(bias_diff, top_diff + n * this->top_dim_);
      }
    }
    //d(loss)/d(weight) and d(loss)/d(input)
    if (this->param_propagate_down_[0] || propagate_down[i])
    {
      const Dtype* bottom_data = bottom[i]->gpu_data();
      Dtype* bottom_diff = bottom[i]->mutable_gpu_diff();

      const Dtype* xnor_bottom_data = this->xnor_bottom_[i]->gpu_data();

      for (int n = 0; n < this->num_; ++n)  //for each batchSize
      {
        // gradient w.r.t. weight. Note that we will accumulate diffs.
        if (this->param_propagate_down_[0])
        {
            //d(loss)/d(weight_xnor)
            this->weight_gpu_gemm(xnor_bottom_data + n * this->bottom_dim_,
                top_diff + n * this->top_dim_, weight_diff);

            //d(loss)/d(weight) cut diff
            gradient_array_ongpu_xnor(weight,conv_out_channels_*kernel_dim_,weight_diff,MAXCUTOFF_W,MINCUTOFF_W);

        }
        // gradient w.r.t. bottom data.
        if (propagate_down[i])
        {
            //d(loss)/d(input_xnor)
            this->backward_gpu_gemm(top_diff + n * this->top_dim_, xnor_weight,
                bottom_diff + n * this->bottom_dim_);

            //d(loss)/d(input)
            gradient_array_ongpu_xnor(bottom_data,num_*bottom_dim_,bottom_diff,MAXCUTOFF_A,MINCUTOFF_A);
        }
      }
    }
  }// top.size
}
```

